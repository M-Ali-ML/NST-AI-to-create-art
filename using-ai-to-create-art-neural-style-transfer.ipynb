{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/mohamedahmedx2/using-ai-to-create-art-neural-style-transfer?scriptVersionId=90205625\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Neural Style transfer\nNST was first introduced in 2015 [paper](https://arxiv.org/abs/1508.06576) it took advantage of how convolution neural network works, The system uses neural representations to separate and recombine content and style of arbitrary images. In simple words, it transfer the style from one image to another. The results vary depending on the style image and the content image. In this notebook, it is designed in a way where you can easily use the Neural Style transfer with your own images with minimum adjustments. *Toggle kitty mode in kaggle notebook for best performance.* ","metadata":{}},{"cell_type":"markdown","source":"# literature review\n\n![VGG19](https://raw.githubusercontent.com/MightyStud/NST-AI-to-create-art/main/diagrams/VGGNet-architecture-19-edited.png)\nUsing VGG19 network with pretrained weights excluding the top fully connected layers, usually for training purposes the weights of different layers are adjusting to minimize the loss function, however in NST, all the layers are frozen(not changed) except the input layer which in that case is the generated image.\n\nThe generated image is free to change its values to minimize the loss function.\n\nHow does the loss function look like in this case, you might ask? The loss function is combination of the content image loss and style image loss. The content loss compares the original image with the generated image for every convoloution layer. The style loss, on the other hand, uses the gram matrix to compare the style loss and the generated image loss.\n\nMore details are explained in the reserch paper.\n","metadata":{}},{"cell_type":"markdown","source":"# results:\nThe results will vary dramatically depending on the parameters you use, you can think of this as a filter for your images but with AI help rather than tools like photoshop.\n![github image of result](https://github.com/MightyStud/NST-AI-to-create-art/blob/main/diagrams/0875.jpg?raw=true)\n\n![github image of result 2](https://raw.githubusercontent.com/MightyStud/NST-AI-to-create-art/main/diagrams/showcase.png)","metadata":{}},{"cell_type":"markdown","source":"[timelapse video on youtube](https://youtu.be/S3XC1HuyxBQ)","metadata":{}},{"cell_type":"markdown","source":"# How to use:\n1.  Upload the 2 folders, a content image folder and the style image folder with your images into your kaggle notebook by creating a new dataset \n2.  enter The name assoiated to your dataset in \"name_of_your_dataset\" variable\n3.  modify the lines of code that has a comment that starts with \"edit:\" in config dictionary to fit your dataset\n4. enable GPU acceleration and go wild","metadata":{}},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"markdown","source":"Importing librares ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Adam, LBFGS\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision import transforms\nimport numpy as np\nimport os\nimport cv2 as cv\nfrom collections import namedtuple\nimport matplotlib.pyplot as plt\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T18:42:35.447383Z","iopub.execute_input":"2022-03-15T18:42:35.447701Z","iopub.status.idle":"2022-03-15T18:42:36.004684Z","shell.execute_reply.started":"2022-03-15T18:42:35.447613Z","shell.execute_reply":"2022-03-15T18:42:36.003794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name_of_your_dataset = \"project-data\" # edit: enter the name of your dataset\nfolders = os.listdir(os.path.join(\"../input\",name_of_your_dataset))\nprint(\"folders[0] is \" ,folders[0],\"\\n folders[1] is \", folders[1])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.006133Z","iopub.execute_input":"2022-03-15T18:42:36.006376Z","iopub.status.idle":"2022-03-15T18:42:36.013676Z","shell.execute_reply.started":"2022-03-15T18:42:36.006342Z","shell.execute_reply":"2022-03-15T18:42:36.01296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"default_resource_dir = os.path.join(\"../input\",name_of_your_dataset)\ncontent_images_dir = os.path.join(default_resource_dir, folders[0]) # edit: index of folders to fit your datasets\nstyle_images_dir = os.path.join(default_resource_dir, folders[1]) # edit: index of folders to fit your datasets\noutput_img_dir = \"./output-images/\"\nimg_format = (4, '.jpg')  # saves images in the format: %04d.jpg","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.015111Z","iopub.execute_input":"2022-03-15T18:42:36.015532Z","iopub.status.idle":"2022-03-15T18:42:36.030045Z","shell.execute_reply.started":"2022-03-15T18:42:36.015496Z","shell.execute_reply":"2022-03-15T18:42:36.029351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initializing the configuration options\nfor quick run, only edit the code cell below. However, I encourge you to dig deeper in the code to understand how it works :)","metadata":{}},{"cell_type":"code","source":"config = {\n    \"content_img_name\":'pyramids3.jpg' , #edit: the name of the content image\n    \"style_img_name\": \"vg_starry_night.jpg\", #edit: the name of style image\n    \"height\" : 1080, #edit: image dimension can be increased however more time and meomory resoureses will be used\n    \"content_weight\" : 1e5 , #edit: can be edited in an experimental way, default = 1e5 , for adam = 1e5 \n    \"style_weight\" : 3e4, #edit: can be edited in an experimental way , default = 3e4 , for adam = 1e5\n    \"tv_weight\" : 1e0, #edit: can be edited in an experimental way , default = 1e0, for adam = 1e-1\n    \"optimizer\" : \"lbfgs\", #edit: adam or lbfgs, adam require less resources however need more epochs(iterations) to output good quality, lbfgs is the opposite(also it was used in the orignal paper) \n    \"epochs\" : 800, #edit: usually 2000 for adam, 800 for lbfgs can be experimented with\n    \"model\" : 'vgg19',\n    \"init_method\" :'content', #edit: can be edited in an experimental way, either 'random', 'content' or 'style'\n    \"saving_freq\" : 200, #edit: note: -1 for only last output\n    \"content_images_dir\" : content_images_dir,\n    \"style_images_dir\" : style_images_dir,\n    \"output_img_dir\" : output_img_dir,  \n    \"img_format\" : img_format,\n    \"lr\" : 5e0, #edit: defaults to 1e0 in lbfgs and 1e1 for adam\n    \"show_results\" : True #edit: False or True to show each image after each save_freq\n} ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.032733Z","iopub.execute_input":"2022-03-15T18:42:36.033489Z","iopub.status.idle":"2022-03-15T18:42:36.04374Z","shell.execute_reply.started":"2022-03-15T18:42:36.033393Z","shell.execute_reply":"2022-03-15T18:42:36.042757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# utilities","metadata":{}},{"cell_type":"markdown","source":"image manipulation ","metadata":{}},{"cell_type":"code","source":"def load_image(img_path, target_shape=None):\n    if not os.path.exists(img_path):\n        raise Exception(f'Path does not exist: {img_path}')\n    img = cv.imread(img_path)[:, :, ::-1]  # [:, :, ::-1] converts BGR (opencv format...) into RGB\n\n    if target_shape is not None:  # resize section, keeping aspect ratio of image intact \n        if isinstance(target_shape, int) and target_shape != -1:  # scalar -> implicitly setting the height\n            current_height, current_width = img.shape[:2]\n            new_height = target_shape\n            new_width = int(current_width * (new_height / current_height))\n            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n        else:  # set both dimensions to target shape\n            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n\n    # this need to go after resizing - otherwise cv.resize will push values outside of [0,1] range\n    img = img.astype(np.float32)  # convert from uint8 to float32\n    img /= 255.0  # get to [0, 1] range\n    return img\n\n# normalize using ImageNet's mean\nIMAGENET_MEAN_255 = [123.675, 116.28, 103.53]\nIMAGENET_STD_NEUTRAL = [1, 1, 1]\ndef prepare_img(img_path, target_shape, device):\n    img = load_image(img_path, target_shape=target_shape)\n\n    \n    # [0, 255] range worked much better for me than [0, 1] range (even though PyTorch models were trained on latter)\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255)),\n        transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)\n    ])\n\n    img = transform(img).to(device).unsqueeze(0)\n\n    return img\n\n\ndef save_image(img, img_path):\n    if len(img.shape) == 2:\n        img = np.stack((img,) * 3, axis=-1)\n    cv.imwrite(img_path, img[:, :, ::-1])  # [:, :, ::-1] converts rgb into bgr (opencv contraint...)\n\n\ndef generate_out_img_name(config):\n    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n    # called from the reconstruction script\n    if 'reconstruct_script' in config:\n        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n    else:\n        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n    return prefix + suffix\n\n\ndef save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False):\n    saving_freq = config['saving_freq']\n    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()\n    out_img = np.moveaxis(out_img, 0, 2)  # swap channel from 1st to 3rd position: ch, _, _ -> _, _, chr\n\n    # for saving_freq == -1 save only the final result (otherwise save with frequency saving_freq and save the last pic)\n    if img_id == num_of_iterations-1 or (saving_freq > 0 and img_id % saving_freq == 0):\n        img_format = config['img_format']\n        out_img_name = str(img_id).zfill(img_format[0]) + img_format[1] if saving_freq != -1 else generate_out_img_name(config)\n        dump_img = np.copy(out_img)\n        dump_img += np.array(IMAGENET_MEAN_255).reshape((1, 1, 3))\n        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n        cv.imwrite(os.path.join(dump_path, out_img_name), dump_img[:, :, ::-1])\n        if should_display:\n            plt.imshow(np.uint8(get_uint8_range(out_img)))\n            plt.show()\n\n\ndef get_uint8_range(x):\n    if isinstance(x, np.ndarray):\n        x -= np.min(x)\n        x /= np.max(x)\n        x *= 255\n        return x\n    else:\n        raise ValueError(f'Expected numpy array got {type(x)}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-15T18:42:36.045414Z","iopub.execute_input":"2022-03-15T18:42:36.046233Z","iopub.status.idle":"2022-03-15T18:42:36.068215Z","shell.execute_reply.started":"2022-03-15T18:42:36.046198Z","shell.execute_reply":"2022-03-15T18:42:36.067437Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model initialization ","metadata":{}},{"cell_type":"code","source":"class Vgg19(torch.nn.Module):\n    \"\"\"\n    Used in the original NST paper, only those layers are exposed which were used in the original paper\n\n    'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1' were used for style representation\n    'conv4_2' was used for content representation (although they did some experiments with conv2_2 and conv5_2)\n    \"\"\"\n    def __init__(self, requires_grad=False, show_progress=False, use_relu=True):\n        super().__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True, progress=show_progress).features\n        if use_relu:  # use relu or as in original paper conv layers\n            self.layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2', 'relu5_1']\n            self.offset = 1\n        else:\n            self.layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1']\n            self.offset = 0\n        self.content_feature_maps_index = 4  # conv4_2\n        # all layers used for style representation except conv4_2\n        self.style_feature_maps_indices = list(range(len(self.layer_names)))\n        self.style_feature_maps_indices.remove(4)  # conv4_2\n\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.slice6 = torch.nn.Sequential()\n        for x in range(1+self.offset):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(1+self.offset, 6+self.offset):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(6+self.offset, 11+self.offset):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(11+self.offset, 20+self.offset):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(20+self.offset, 22):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(22, 29++self.offset):\n            self.slice6.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        x = self.slice1(x)\n        layer1_1 = x\n        x = self.slice2(x)\n        layer2_1 = x\n        x = self.slice3(x)\n        layer3_1 = x\n        x = self.slice4(x)\n        layer4_1 = x\n        x = self.slice5(x)\n        conv4_2 = x\n        x = self.slice6(x)\n        layer5_1 = x\n        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n        out = vgg_outputs(layer1_1, layer2_1, layer3_1, layer4_1, conv4_2, layer5_1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.069769Z","iopub.execute_input":"2022-03-15T18:42:36.070276Z","iopub.status.idle":"2022-03-15T18:42:36.08822Z","shell.execute_reply.started":"2022-03-15T18:42:36.07024Z","shell.execute_reply":"2022-03-15T18:42:36.087496Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"call model","metadata":{}},{"cell_type":"code","source":"def prepare_model(model, device):\n    # we are not tuning model weights -> we are only tuning optimizing_img's pixels! (that's why requires_grad=False)\n    \n    if model == 'vgg19':\n        model = Vgg19(requires_grad=False, show_progress=True)\n    #else: model == \"Your model\" #uncomment and experiment with another model\n\n    content_feature_maps_index = model.content_feature_maps_index\n    style_feature_maps_indices = model.style_feature_maps_indices\n    layer_names = model.layer_names\n\n    content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])\n    style_fms_indices_names = (style_feature_maps_indices, layer_names)\n    return model.to(device).eval(), content_fms_index_name, style_fms_indices_names\n\n\ndef gram_matrix(x, should_normalize=True):\n    (b, ch, h, w) = x.size()\n    features = x.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t)\n    if should_normalize:\n        gram /= ch * h * w\n    return gram\n\n\ndef total_variation(y):\n    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.08992Z","iopub.execute_input":"2022-03-15T18:42:36.090148Z","iopub.status.idle":"2022-03-15T18:42:36.102672Z","shell.execute_reply.started":"2022-03-15T18:42:36.090124Z","shell.execute_reply":"2022-03-15T18:42:36.101899Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"main script\n","metadata":{}},{"cell_type":"code","source":"def build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n    target_content_representation = target_representations[0]\n    target_style_representation = target_representations[1]\n\n    current_set_of_feature_maps = neural_net(optimizing_img)\n\n    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)\n    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n\n    style_loss = 0.0\n    current_style_representation = [gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n    style_loss /= len(target_style_representation)\n\n    tv_loss = total_variation(optimizing_img)\n\n    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss\n\n    return total_loss, content_loss, style_loss, tv_loss\n\n\ndef make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n    # Builds function that performs a step in the tuning loop\n    def tuning_step(optimizing_img):\n        total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config)\n        # Computes gradients\n        total_loss.backward()\n        # Updates parameters and zeroes gradients\n        optimizer.step()\n        optimizer.zero_grad()\n        return total_loss, content_loss, style_loss, tv_loss\n\n    # Returns the function that will be called inside the tuning loop\n    return tuning_step","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.104072Z","iopub.execute_input":"2022-03-15T18:42:36.10438Z","iopub.status.idle":"2022-03-15T18:42:36.116247Z","shell.execute_reply.started":"2022-03-15T18:42:36.104341Z","shell.execute_reply":"2022-03-15T18:42:36.115383Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def neural_style_transfer(config):\n    content_img_path = os.path.join(config['content_images_dir'], config['content_img_name'])\n    style_img_path = os.path.join(config['style_images_dir'], config['style_img_name'])\n\n    out_dir_name = 'combined_' + os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0] + '_'+ str(config[\"height\"])\n    dump_path = os.path.join(config['output_img_dir'], out_dir_name)\n    os.makedirs(dump_path, exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    content_img = prepare_img(content_img_path, config['height'], device)\n    style_img = prepare_img(style_img_path, config['height'], device)\n\n    if config['init_method'] == 'random':\n        # white_noise_img = np.random.uniform(-90., 90., content_img.shape).astype(np.float32)\n        gaussian_noise_img = np.random.normal(loc=0, scale=90., size=content_img.shape).astype(np.float32)\n        init_img = torch.from_numpy(gaussian_noise_img).float().to(device)\n    elif config['init_method'] == 'content':\n        init_img = content_img\n    else:\n        # init image has same dimension as content image - this is a hard constraint\n        # feature maps need to be of same size for content image and init image\n        style_img_resized = prepare_img(style_img_path, np.asarray(content_img.shape[2:]), device)\n        init_img = style_img_resized\n\n    # we are tuning optimizing_img's pixels! (that's why requires_grad=True)\n    optimizing_img = Variable(init_img, requires_grad=True)\n\n    neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = prepare_model(config['model'], device)\n    print(f'Using {config[\"model\"]} in the optimization procedure.')\n\n    content_img_set_of_feature_maps = neural_net(content_img)\n    style_img_set_of_feature_maps = neural_net(style_img)\n\n    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)\n    target_style_representation = [gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]\n    target_representations = [target_content_representation, target_style_representation]\n\n    # magic numbers in general are a big no no - some things in this code are left like this by design to avoid clutter\n    num_of_iterations = {\n        \"lbfgs\": config['epochs'],\n        \"adam\": config['epochs'],\n    }\n\n    #\n    # Start of optimization procedure\n    #\n    if config['optimizer'] == 'adam':\n        optimizer = Adam((optimizing_img,), lr=config[\"lr\"])\n        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n        for cnt in range(num_of_iterations[config['optimizer']]):\n            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)\n            with torch.no_grad():\n                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n                save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=config[\"show_results\"])\n                \n                \n    elif config['optimizer'] == 'lbfgs':\n        # line_search_fn does not seem to have significant impact on result\n        optimizer = LBFGS((optimizing_img,), lr=config[\"lr\"], max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe') # lr = 1 the default\n        cnt = 0\n\n        def closure():\n            nonlocal cnt\n            if torch.is_grad_enabled():\n                optimizer.zero_grad()\n            total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n            if total_loss.requires_grad:\n                total_loss.backward()\n            with torch.no_grad():\n                print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n                save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=config[\"show_results\"])\n\n            cnt += 1\n            return total_loss\n\n        optimizer.step(closure)\n\n    return dump_path","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.11959Z","iopub.execute_input":"2022-03-15T18:42:36.119923Z","iopub.status.idle":"2022-03-15T18:42:36.143787Z","shell.execute_reply.started":"2022-03-15T18:42:36.119884Z","shell.execute_reply":"2022-03-15T18:42:36.143001Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run statement","metadata":{}},{"cell_type":"code","source":"results_path = neural_style_transfer(config)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T18:42:36.146915Z","iopub.execute_input":"2022-03-15T18:42:36.147188Z","iopub.status.idle":"2022-03-15T18:50:37.974058Z","shell.execute_reply.started":"2022-03-15T18:42:36.147161Z","shell.execute_reply":"2022-03-15T18:50:37.973373Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"post-processing","metadata":{}},{"cell_type":"code","source":"#to zip all the files for download uncomment the next line of code\nshutil.make_archive(\"Neural Style transfere complete0\", 'zip', output_img_dir)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-15T18:50:37.980526Z","iopub.execute_input":"2022-03-15T18:50:37.98096Z","iopub.status.idle":"2022-03-15T18:50:38.375752Z","shell.execute_reply.started":"2022-03-15T18:50:37.980924Z","shell.execute_reply":"2022-03-15T18:50:38.374929Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete current output\n#shutil.rmtree(\"./output-images\")\n#os.remove(\"./Neural Style transfere complete2.zip\")\n\n#memory cleaning\n#import gc\n#gc.collect()\n\n#empty GPU memory\n#torch.cuda.empty_cache()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T19:02:07.304373Z","iopub.execute_input":"2022-03-15T19:02:07.304962Z","iopub.status.idle":"2022-03-15T19:02:07.30854Z","shell.execute_reply.started":"2022-03-15T19:02:07.304925Z","shell.execute_reply":"2022-03-15T19:02:07.307751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References and heavy influencers:\n+ [A Neural Algorithm of Artistic Style (paper)](https://arxiv.org/abs/1508.06576)\n+ [Image Style Transfer Using Convolutional Neural Networks (paper)](https://ieeexplore.ieee.org/document/7780634)\n+ [Real-Time Neural Style Transfer for Videos (paper)(feedforward approach)](https://ieeexplore.ieee.org/document/8100228)\n+ [TheAIEpiphany youtube channel](https://www.youtube.com/c/TheAIEpiphany)\n+ [gordicaleksa's repo](https://github.com/gordicaleksa/pytorch-neural-style-transfer)\n+ [Aladdin Persson youtube video](https://www.youtube.com/watch?v=imX4kSKDY7s&t=1236s)\n+ [Tensorflow NST tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer#total_variation_loss)\n","metadata":{}}]}